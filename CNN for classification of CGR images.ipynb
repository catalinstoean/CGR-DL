{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from __future__ import print_function\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D\n",
    "from tensorflow.keras.layers import Activation, Dropout, Flatten, Dense\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from sklearn import metrics\n",
    "import os\n",
    "import glob  # to count jpg files\n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Dropout, Flatten\n",
    "from tensorflow.keras import regularizers, optimizers\n",
    "import keras\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "from tensorflow.keras.applications import VGG19, ResNet50, xception, inception_v3\n",
    "\n",
    "import time\n",
    "import winsound  # to play a sound when the program finishes\n",
    "\n",
    "\n",
    "def createSimpleModelTune(dr1, dr3, noOfFilters, kernelSize):\n",
    "    model = Sequential()\n",
    "    # The first two layers with 32 filters of window size 3x3\n",
    "    model.add(Conv2D(noOfFilters, (kernelSize, kernelSize), padding='same', activation='relu', input_shape=(128, 128, 3)))#model.add(Conv2D(8, (7, 7), padding='same', activation='relu', input_shape=(128, 128, 3)))\n",
    "    # model.add(Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(dr1))  # 0.25\n",
    "\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dropout(dr3))  # 0.9\n",
    "    model.add(Dense(10, activation='softmax',\n",
    "                    kernel_regularizer=regularizers.l1(0.01),\n",
    "                    activity_regularizer=regularizers.l1(0.01)))\n",
    "\n",
    "    return model\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=45)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    #plt.savefig('D:/cfValCCAT10.pdf', bbox_inches = 'tight')\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def CCAT10Case(justLoadPreviousModel = True):\n",
    "\n",
    "\n",
    "    image_size = 128\n",
    "    noOfRuns = 1\n",
    "    epochs = 1000\n",
    "    batch_size = 32\n",
    "    '''\n",
    "    # dropouts initial CCAT\n",
    "    dr1 = 0.3\n",
    "    dr2 = 0.6\n",
    "    dr3 = 0.5\n",
    "    '''\n",
    "    # dropouts from IMDB\n",
    "    dr1 = 0.3\n",
    "    dr2 = 0.6\n",
    "    dr3 = 0.8\n",
    "\n",
    "    mainPath = 'D:\\\\Datasets\\\\CCAT\\\\CCAT-GAN\\\\'\n",
    "    trainPath = mainPath + 'train'\n",
    "    validPath = mainPath + 'valid'\n",
    "    testPath = mainPath + 'test'\n",
    "\n",
    "    jpgCounterTrain = len(glob.glob(trainPath + \"/**/*.jpg\")) + len(glob.glob(trainPath + \"/**/*.png\")) + len(glob.glob(trainPath + \"/**/*.tiff\"))\n",
    "    jpgCounterValid = len(glob.glob(validPath + \"/**/*.jpg\")) + len(glob.glob(validPath + \"/**/*.png\")) + len(glob.glob(validPath + \"/**/*.tiff\"))\n",
    "    jpgCounterTest = len(glob.glob(testPath + \"/**/*.jpg\")) + len(glob.glob(testPath + \"/**/*.png\")) + len(glob.glob(testPath + \"/**/*.tiff\"))\n",
    "\n",
    "\n",
    "\n",
    "    lossValue = 'categorical_crossentropy'\n",
    "    classModeValue = 'categorical'\n",
    "\n",
    "    start_time = time.time()\n",
    "    i = 0\n",
    "    dr1 = 0.5\n",
    "    dr3 = 0.3\n",
    "    noOfFilters = 16\n",
    "    kernelSize = 7\n",
    "    '''\n",
    "    for dr1 in [0.1, 0.5]:\n",
    "        for dr3 in [0.1, 0.3]:\n",
    "            for noOfFilters in [4, 8, 16, 32]:\n",
    "                for kernelSize in [3, 5, 7, 9]:\n",
    "    '''\n",
    "    #model1 = createSimpleModel(dr1, dr3)\n",
    "    model1 = createSimpleModelTune(dr1, dr3, noOfFilters, kernelSize)\n",
    "\n",
    "    model1.compile(loss=lossValue, optimizer=\"rmsprop\",\n",
    "                   metrics=['acc'])  # 1e-6 #optimizers.RMSprop(lr=1e-6, momentum=0.9)\n",
    "    # this is the augmentation configuration we will use for training\n",
    "    train_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    # this is the augmentation configuration we will use for testing:\n",
    "    # only rescaling\n",
    "    valid_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    test_datagen = ImageDataGenerator(rescale=1. / 255)\n",
    "    seed = 5\n",
    "\n",
    "    # this is a generator that will read pictures found in\n",
    "    # subfolDers of 'data/train', and indefinitely generate\n",
    "    # batches of augmented image data\n",
    "    train_generator = train_datagen.flow_from_directory(\n",
    "        trainPath,  # this is the target directory\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        #color_mode='grayscale',\n",
    "        class_mode=classModeValue, seed=seed,\n",
    "        shuffle=True)  # since we use binary_crossentropy loss, we need binary labels\n",
    "\n",
    "    # this is a similar generator, for validation data\n",
    "    validation_generator = valid_datagen.flow_from_directory(\n",
    "        validPath,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=batch_size,\n",
    "        #color_mode='grayscale',\n",
    "        class_mode=classModeValue, shuffle=False)\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        testPath,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=jpgCounterTest,\n",
    "        #color_mode='grayscale',\n",
    "        class_mode=classModeValue, shuffle=False)\n",
    "\n",
    "    outputFolder = 'D:\\\\Datasets\\\\Saved2020Results\\\\CCAT-2020-Revision\\\\'\n",
    "    # create the folder\n",
    "    if not os.path.exists(outputFolder):\n",
    "        os.makedirs(outputFolder)\n",
    "\n",
    "    filepath = outputFolder + \"AugmentedWeightsBestRunCCAT10.hdf5\"\n",
    "    checkpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, save_best_only=True, mode='max')\n",
    "    callbacks_list = [EarlyStopping(monitor='val_acc', patience=50, verbose=0), checkpoint]\n",
    "\n",
    "    if justLoadPreviousModel == False:\n",
    "        history2 = model1.fit_generator(\n",
    "            train_generator,\n",
    "            steps_per_epoch=jpgCounterTrain // batch_size,\n",
    "            epochs=epochs,\n",
    "            validation_data=validation_generator,\n",
    "            callbacks=callbacks_list,\n",
    "            validation_steps=jpgCounterValid // batch_size)\n",
    "\n",
    "        plt.figure(figsize=[8, 6])\n",
    "        plt.plot(history2.history['loss'], 'r', linewidth=3.0)\n",
    "        plt.plot(history2.history['val_loss'], 'b', linewidth=3.0)\n",
    "        plt.legend(['Training loss', 'Validation Loss'], fontsize=18)\n",
    "        plt.xlabel('Epochs ', fontsize=16)\n",
    "        plt.ylabel('Loss', fontsize=16)\n",
    "        plt.title('Loss Curves', fontsize=16)\n",
    "        plt.show()\n",
    "\n",
    "        # Accuracy Curves\n",
    "        plt.figure(figsize=[8, 6])\n",
    "        plt.plot(history2.history['acc'], 'r', linewidth=3.0)\n",
    "        plt.plot(history2.history['val_acc'], 'b', linewidth=3.0)\n",
    "        plt.legend(['Training Accuracy', 'Validation Accuracy'], fontsize=18)\n",
    "        plt.xlabel('Epochs ', fontsize=16)\n",
    "        plt.ylabel('Accuracy', fontsize=16)\n",
    "        plt.title('Accuracy Curves', fontsize=16)\n",
    "        plt.show()\n",
    "    elapsedTrainingTime = time.time() - start_time\n",
    "\n",
    "    model1.load_weights(filepath)\n",
    "\n",
    "    xTest, yTest = test_generator.next()\n",
    "    print('class_indices = ', test_generator.class_indices)\n",
    "    evaluation = model1.evaluate(xTest, yTest)\n",
    "    print(\"Loss and evaluation on the test set:\", evaluation)\n",
    "\n",
    "    if justLoadPreviousModel == False:\n",
    "    # save results to file\n",
    "    # import os.path\n",
    "        outputFileName = outputFolder + '\\AugmentedResultsCNNKernelTuneFiltersKS.txt'\n",
    "        if os.path.isfile(outputFileName):  # if the file exists, add to it, otherwise create it\n",
    "            f = open(outputFileName, 'a')\n",
    "\n",
    "            f.write(str(i) + '\\t' + str(dr1) + '\\t' + str(dr2) + '\\t' + str(dr3)\n",
    "                    + '\\t' + str(noOfFilters) + '\\t' + str(kernelSize)\n",
    "                    + '\\t' + str(epochs)\n",
    "                    + '\\t' + str(batch_size) + '\\t' + str(image_size)\n",
    "                    + '\\t' + str(noOfRuns) + '\\t' + str(jpgCounterValid) + '\\t' + str(jpgCounterTest)\n",
    "                    + '\\t' + str(evaluation[0]) + '\\t' + str(evaluation[1])\n",
    "                    + '\\t' + str(min(history2.history['loss'])) + '\\t' + str(max(history2.history['acc']))\n",
    "                    + '\\t' + str(min(history2.history['val_loss'])) + '\\t' + str(max(history2.history['val_acc']))\n",
    "                    + '\\t' + str(elapsedTrainingTime) + '\\n')\n",
    "            f.close()  # I did not test if these close() are necessary. It works without them\n",
    "        else:\n",
    "            f = open(outputFileName, 'w')\n",
    "            f.write(\n",
    "                'currentRun\\tdr1\\tdr2\\tdr3\\t\\tnoOfFilters\\tkernelSize\\tepochs\\tbatch_size\\timage_size\\tnoOfRuns\\tvalidationImages\\ttestImages\\ttestLoss\\ttestAccuracy\\tminTrainLoss\\tmaxTrainAcc\\tminValidLoss\\tmaxValidAcc\\telapsedTrainingTime\\n')\n",
    "            f.close()\n",
    "            f = open(outputFileName, 'a')\n",
    "            f.write(str(i) + '\\t' + str(dr1) + '\\t' + str(dr2) + '\\t' + str(dr3)\n",
    "                    + '\\t' + str(noOfFilters) + '\\t' + str(kernelSize)\n",
    "                    + '\\t' + str(epochs)\n",
    "                    + '\\t' + str(batch_size) + '\\t' + str(image_size)\n",
    "                    + '\\t' + str(noOfRuns) + '\\t' + str(jpgCounterValid) + '\\t' + str(jpgCounterTest)\n",
    "                    + '\\t' + str(evaluation[0]) + '\\t' + str(evaluation[1])\n",
    "                    + '\\t' + str(min(history2.history['loss'])) + '\\t' + str(max(history2.history['acc']))\n",
    "                    + '\\t' + str(min(history2.history['val_loss'])) + '\\t' + str(max(history2.history['val_acc']))\n",
    "                    + '\\t' + str(elapsedTrainingTime) + '\\n')\n",
    "            f.close()\n",
    "        # save acc and loos for train and validation during evolution\n",
    "        outputFileRuntime = outputFolder + '\\AugmentedMoreResultsRuntimeCNN' + str(dr1) + str(dr2) + str(dr3) + '.txt'\n",
    "        print('The following file is saved: ', outputFileRuntime)\n",
    "        np.savetxt(outputFileRuntime, np.c_[\n",
    "            history2.history['loss'], history2.history['val_loss'], history2.history['acc'], history2.history[\n",
    "                'val_acc']])\n",
    "\n",
    "    # %%\n",
    "\n",
    "    entireValidationGenerator = valid_datagen.flow_from_directory(\n",
    "        validPath,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=jpgCounterValid,\n",
    "        #color_mode='grayscale',\n",
    "        class_mode=classModeValue, shuffle=False)\n",
    "\n",
    "    xVal, yVal = entireValidationGenerator.next()\n",
    "\n",
    "    test_generator = test_datagen.flow_from_directory(\n",
    "        testPath,\n",
    "        target_size=(image_size, image_size),\n",
    "        batch_size=jpgCounterTest,\n",
    "        #color_mode='grayscale',\n",
    "        class_mode=classModeValue, shuffle=False)\n",
    "    xTest, yTest = test_generator.next()\n",
    "\n",
    "    evaluationVal = model1.evaluate(xVal, yVal)\n",
    "    print(\"Loss and evaluation on the validation set:\", evaluationVal)\n",
    "    predictionsVal = model1.predict(xVal)\n",
    "\n",
    "    predictionsTest = model1.predict(xTest)\n",
    "    evaluationTest = model1.evaluate(xTest, yTest)\n",
    "    print(\"Loss and evaluation on the test set:\", evaluationTest)\n",
    "\n",
    "    cmVal = confusion_matrix(yVal.argmax(1), predictionsVal.argmax(axis=-1))\n",
    "    print('Val CF:\\n', cmVal)\n",
    "\n",
    "    cmTest = confusion_matrix(yTest.argmax(1), predictionsTest.argmax(axis=-1))\n",
    "    print('Test CF:\\n', cmTest)\n",
    "    print('Evaluation of validation')\n",
    "    print(metrics.classification_report(yVal.argmax(1), predictionsVal.argmax(axis=-1), digits=3))\n",
    "    print('Evaluation of test')\n",
    "    print(metrics.classification_report(yTest.argmax(1), predictionsTest.argmax(axis=-1), digits=3))\n",
    "    cmVal = confusion_matrix(yVal.argmax(1), predictionsVal.argmax(axis=-1))\n",
    "    cm_plot_labels = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "    plot_confusion_matrix(cmVal, cm_plot_labels, title='Confusion Matrix Validation Set')\n",
    "\n",
    "    cmTest = confusion_matrix(yTest.argmax(1), predictionsTest.argmax(axis=-1))\n",
    "    cm_plot_labels = ['1', '2', '3', '4', '5', '6', '7', '8', '9', '10']\n",
    "    plot_confusion_matrix(cmTest, cm_plot_labels, title='Confusion Matrix Test Set')\n",
    "    print('Actual outputs:', yTest.argmax(1))\n",
    "    print('Predicted outputs:', predictionsTest.argmax(axis=-1))\n",
    "    print('dr1 = {}, dr3 = {}'.format(dr1, dr3))\n",
    "    keras.backend.clear_session()\n",
    "    return evaluationTest\n",
    "\n",
    "evaluationTest = CCAT10Case(False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
